---
permalink: /
title: "Wenwei Zhang, 张文蔚"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Wenwei Zhang is a Researcher at [Shanghai Artificial Intelligence Laboratory](https://www.shlab.org.cn/). He obtained his Ph.D. degree at [MMLab](https://www.mmlab-ntu.com/), [School of Computer Science and Engineering](http://scse.ntu.edu.sg/Pages/Home.aspx), Nanyang Technological University, Singapore, supervised by Professor [Chen Change (Cavan) Loy](https://www.mmlab-ntu.com/person/ccloy/index.html).
Before that, he received his bachelor degree at the [Computer Science School](http://cs.whu.edu.cn/), Wuhan University.
You can find his CV [here](/files/resume.pdf).

He has been leading the fine-tuning team of [InternLM](https://internlm.intern-ai.org.cn/) since 2023, working on AI Agents and self-improvement (scalable oversight) of Large Language Models.
His past works mainly explore [versatile neural architectures across modalities and perception tasks](https://dr.ntu.edu.sg/handle/10356/171935) and open-vocabulary perception.
He built and released [MMDetection3D](https://github.com/open-mmlab/mmdetection3d), and has been leading the development of [MMDetection](https://github.com/open-mmlab/mmdetection) and [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) since 2020, respectively, as a core member of [OpenMMLab](https://openmmlab.com/) since 2019.

Recent News
------------------------

2 papers ([EmbodiedScan](https://arxiv.org/abs/2312.16170) and [OMG-Seg](https://github.com/lxtGH/OMG-Seg)) are accepted by CVPR2024. (Feb. 2024)

InternLM2-Chat, state-of-the-art open-source LLMs in [opencompass](https://rank.opencompass.org.cn/home) and [AlpacaEval2](https://tatsu-lab.github.io/alpaca_eval/), have been released. See [InternLM2 Model Zoo](https://github.com/InternLM/InternLM?tab=readme-ov-file#model-zoo). (Jan. 2024)

2 papers ([CLIPSelf](https://arxiv.org/abs/2310.01403) and [UniHSI](https://arxiv.org/abs/2309.07918)) are accepted as <font color="Tomato"><strong>spotlights</strong></font> by ICLR2024. (Jan. 2024)

2 papers ([Robo3D](https://arxiv.org/abs/2303.17597) and [Tube-Link](https://arxiv.org/abs/2303.12782)) are accepted by ICCV2023. (July. 2023)

3 papers ([BARON](https://arxiv.org/abs/2302.13996), [MV-JAR](https://arxiv.org/abs/2303.13510), and [DDQ](https://arxiv.org/abs/2303.12776)) are accepted by CVPR2023. (Mar. 2023)

We release [OpenMMLab 2.0](https://openmmlab.com/) with a new core, [MMEngine](https://github.com/open-mmlab/mmengine). (Sept. 2022)

[Video K-Net](https://arxiv.org/abs/2204.04656) is accepted by CVPR 2022 (<font color="Tomato"><strong>oral</strong></font>). (May., 2022)

[K-Net](https://www.mmlab-ntu.com/project/knet/index.html) is accepted by NeurIPS 2021. Code has been released at [here](https://github.com/ZwwWayne/K-Net). (Oct., 2021)

MMDet3D team obtains <font color="Tomato"><strong>Best PKL Award</strong></font> and best vision-only results in the 3rd nuScenes detection challenge of 5th AI Driving Olympics, NeurIPS 2020.
Paper of our multi-modality method is released in [arxiv](https://arxiv.org/abs/2012.12741). (Dec., 2020)

Second runner up in [LVIS2020 Challenge](https://www.lvisdataset.org/challenge_2020). Paper can be found [in arxiv](https://arxiv.org/abs/2008.10032).

We release [MMDetection3D](https://github.com/open-mmlab/mmdetection3d), OpenMMLab's next-generation platform for general 3D object detection. (July, 2020)

Win the <font color="Tomato"><strong>1st prize</strong></font> in [COCO 2019 Object Detection Challenge](http://cocodataset.org/workshop/coco-mapillary-iccv-2019.html) (no external data). (Team: MMDet)

Academic Service
------------------------

Conference Reviewer: CVPR2020-2024, ICCV2021-2023, ECCV2020-2024, ICLR2022-2024, NeurIPS2021-2023, ICML2023-2024, ACM MM2020.

Committee member and speaker of OpenMMLab Tutorials in CVPR [2021](https://openmmlab.com/community/cvpr2021-tutorial)/[2022](https://openmmlab.com/community/cvpr2022-tutorial)/2023, and [AAAI2023](https://openmmlab.com/community/aaai2023-lab)
